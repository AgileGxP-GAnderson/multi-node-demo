# NATS Monitoring System

This project implements a multi-node monitoring system using NATS for messaging. It allows for the management and monitoring of nodes in a distributed environment.

## Project Structure

```
multi-node-demo
├── src
│   ├── index.ts               # Entry point of the application
│   ├── config
│   │   └── natsConfig.ts      # Configuration for NATS connection
│   ├── nodes
│   │   └── Manager.ts      # Manages nodes in the system
│   ├── monitoring
│   │   └── health-monitor.ts   # Monitors the nodes
│   └── types
│       └── index.ts            # Type definitions for nodes and monitoring config
├── package.json                # NPM package configuration
├── tsconfig.json               # TypeScript configuration
└── README.md                   # Project documentation
```

## Installation
1. Clone the repository:
   ```
   git clone https://github.com/AgileGxP-GAnderson/multi-node-demo.git
   
   ```

2. Install the dependencies:
   ```
   npm install
   ```

## Usage

To start the monitoring system, run the following commands:
```
-- Start NATS
nats-server.exe

-- Start engine wrappers.  Name as you will
npx ts-node src\nodes\engine-wrapper.ts --ENGINE_ID engine1
npx ts-node src\nodes\engine-wrapper.ts --ENGINE_ID engine2

-- Start subscriber 
npx ts-node src\nodes\subscriber.ts

-- Start publisher
npx ts-node src\nodes\publisher.ts

-- Start health monitor
npx ts-node src\monitoring\health-monitor.ts

-- Optional : to 'spy' on NATS message for troubleshooting:
npx ts-node src\nodes\sniffer.ts

Enhanced explaination of model - Glenn 5/14/2025

This is a small POC regarding how the Agile engine could be deployed and act in a 'multi-node' redundant manner.  It's not using the Agile engine itself in this demo, but rather 'engine-wrapper' nodes that process messages from a publisher and pass them to a subscriber.  The nodes themselves use NATS.  The inputs and outputs to these nodes are also NATS messages but could be trivially replaced with a different input or output type.

Start the app as instructed above.

The 'publisher' simply publishes a message every second to NATS with a timestamp.

Each 'engine-wrapper' is started with a unique name as the --ENGINE_ID.  They all subscribe to the message being generated by the publisher and when hooked to a real Agile Engine in the future, will process this message through all layers.  The difference is that the 'leader' will publish the final message to the subscriber, while the non-leaders will buffer these messages.  There can only be one leader.  All non-leaders are monitoring the health (heartbeats) of the leader and volunteer to become the leader themselves if too much time between heartbeats happens.

So here is a realistic use scenario.  A single engine-wrapper node is started.  It doesn't know how many other engine-wrappers may exist, if any.  It does know that the leader will broadcast it's leadership frequently via NATS.  This engine-wrapper waits for a couple of seconds to see if another node is proclaiming it's leadership.  If so, it will recognize the leader and will buffer its output messages rather than publish them.  If it hears no messages from a leader, it will attempt to become the leader itself.

If an engine-node attempts to become the leader, it's possible that others will too.  All nodes agree that in this case, the first request to be leader (which has a timestamp) will be accepted as the new leader.  If the timestamps are identical, the engine name first in alphabetical order will become the leader.

If a new leader takes over, it will immediately publish all messages it had in it's buffer from it's time as a non-leader, and then will publish messages as normal.

There's also a health monitor which just publishes the number of 'healthy' nodes, as well as which node is the leader.

The 'sniffer' just subscribes to all the cross-node messages that are being published and prints them to the console.  It was useful to see the traffic when developing this.

That's it.  This could be a good starting point to adding this multi-node feature to Agile Engine.

Some edge cases that might be considered:

* The 'buffer' that is dumped after a new leader is elected may contain messages that had been sent before, or may be too small to have held all messages that happened when the former leader was down and a new leader elected.  Some examination of timestamps might be in order to minimize duplicate messages while ensuring none are lost.

* There could be a case where the leader is sending it's status to the other nodes, but isn't heard due to network issues, etc.  In this case, other nodes would hold a new leader election with the original leader still thinking that it is the leader.  In this case, some thought as to how this would be handled is warranted.

The code is fairly elegant and easy enough to understand I think.  Feel free to contact me for clarification or ideas as to how to proceeed.




